---
title: "PHU Data Specialist Instruction/Guidelines Handbook"
author: "Abraham Azar"
date: last-modified
---
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script>
  $(document).ready(function () {
    $(".abstract").hide();
    $(".button").on("click", function () {
        $(this).next(".abstract").slideToggle(400);
    });
});
</script>

<style>
.abstract{text-align:justify; }
.button{ text-align:justify; }
</style>


# Introduction

This handbook is dedicated for the next Data Specialist taking over the position within the Public Health Unit in the Global Programmes department at HQ. The book dives into each aspect of the position, showcasing step by step guidances on how to run the existing tools and how to maintain them. 

I will try my best to be as detailed as possible, but do know that other data/technical people will also have answers to all your questions.

# Tools

## impactR4PHU

[healthyR](https://github.com/SaeedR1987/healthyr), a tool built by Saeed Rahman (current PHU Unit Manager), has been a crucial tool for checking, cleaning and analysing the data of almost all the outcome indicators of the different sectors of Public Health (Health/Nutrition/Food Security & Livelihoods - FSL/WASH). 

[impactR4PHU](https://github.com/impact-initiatives/impactR4PHU) followed the same original content structure of heathyR, but addressed some of the feedback from the users, as well as some other bugs/issues. The package also introduced project templates for fast implementations of the processes. 

This package is following a [Test-Driven Development](https://www.techtarget.com/searchsoftwarequality/definition/test-driven-development) where unit testing, programming, and refactoring of the source code is addapted during development stages. 

### Content

Here is a quick dissection of the GitHub Repo, but please do check the READ ME Section for a better description of each component. 

#### **main branch**

##### **R Folder**

This folder includes all the functions that are used by the Public Health Unit for *creating*, *adding*, *checking*, and *ploting* data within different processes. Almost all functions have a test unit script using the [testthat](https://testthat.r-lib.org/) package. The tests can be access through the testing branch. Some of the functions are helpers functions and do not include testers. 
    
All functions are built in a way that can be piped within the **dplyr** system of piping.

Once you have checked all the functions, you might ask yourself a question. 

> *"Where are the functions related to Health and WASH?"* 

They exist, but not in this package. At Impact, we love to share tasks sometimes, mainly when it is used for specific purposes in other units. You will be able to find these respective functions in the [humind](https://github.com/impact-initiatives-hppu/humind) package. The humind package is built to address main calculations for indicators collected through the MSNA (Multi-Sectoral Needs Assessment). To know more about this assessment or the package, please reach out to the HPPU Unit colleagues.

All functions are well documented using Roxygen, and some funcitons are running some examples using dummy data included in the package. 

##### **data Folder**

This folder includes different dummy datasets used within the examples inside the functions, but also within the test units. 

##### **inst Folder**

This folder includes different project templates:

- FSL: Food Security and Livelihoods.
- IYCF: Infant Youth Consumption Food TO CHECK
- Mortality
- IPHRA: Integrated Public Health Rapid Assessment
- Public Health integrated tables

You might ask yourself another question here.

> *Why do we have templates if there are already functions?*

Good question!! But here is the answer.

Impact's field missions are widely different when it come to data resources, capacity, and knowledge. Some missions do not even have data officers. So these templates exists to make the life of users with little knowledge of R easier and faster. They only need to set up R and R Studio on their machines, and run the respective scripts. 

These project templates can easily be accessed through the "New Project" button in R Studio after installing the Package

> devtools::install_github("impact-initiatives/impactR4PHU")

The scripts interacts with the users through inputs system. Inputs here are very important to make sure that the scripts are outputing the correct intentions.

Please make sure to refer back to the respective specialists within the Public Health Unit so they can help you with what to select what when it comes to running the scripts.

- Mortality: Noortje Gerritzma
- FSL: Olivia Falkowitz
- IYCF: Martin 
- IPHRA: PH team
- PH Integrated Tables: PH team or Olivia. 

Mortality, FSL, and IYCF have 3 templates:

- Plausibility and Quality Reports
- Cleaning
- Descriptive analysis.

IPHRA have 2 templates:

- Cleaning
- Analysis

PH integrated Tables have only 1 template. 

All the details needed are explained in the READ ME sections of the github page. 

#### **testing branch**

##### **tests Folder** 

Using the testthat package, and followint the TDD method, all functions have been tested following the good and bad path to make sure that they are outputing the expected results. Please do ensure to build tests always with every added functions, and feel free to add tests to existing functions. 

### Maintenance

If changes, improvements, enhancements, add-ons are required to the package, please use the following process to ensure a smooth merging to the main branch. 

First, clone the main package to your local machine and create a new branch for your work. 
If functions are created/ammended, **please** make sure to checkout to the testing branch and do the necessary work there to keep the package trusted and correctly functioning. 

Once done, go ahead commit and push your changes to your respective branch, and open a pull requests. 

Once you create the pull request, some Github Actions will run in teh background to ensure that the package is functional and installable in different operating systems and the coverage test badge is updated. If all the actions pass the tests, I set a requirement here to have a review and validation by another person. However, as a solo data person in the unit, this can be bypassed and merge can be confirmed. 

Github will again run another test of the Actions and pass a green badge to the R CMD Check. 

> The job is not yet done !!

As all the templates are using there own specific renv environment that also include the impactR4PHU github package, all these renv of the different templates should be updated to the newest version of the package. 

I created a python script that update them automatically instead of accessing them one by one. You will find the link to the python script inside the handover folder in the Sharepoint. 

Once the script is done, please go again to your local branch where the new changes have been made and you will realise that all renv.lock files have now updated and require a new commit, push and pull for github. I know this is a bit of an annoying process, please go ahead and change it to your convenience. 

### Requirements

- Please inform users to always re-install the package so they can access the latest version, especially if you are constantly enhancing/improving/changing something within the package. This is required because of the renv.locks that includes all newest packages requirements. 

- Each template have a renv::update() function in the beginning which is responsible of updating the relevant packages to the latest requirements for the dependencies. However, R sometimes love to be hard and try to break everything. I realized this after receiving some weird **"make error"** from the field. Kindly keep an eye on that. 

### Next Steps Suggestions

As the rotation within Impact is relatively high, you should consider having regular refreshers for missions/HQ Global Programs (GP) and Research Department (RDD) on the usage of the package and mainly the templates. 

> *"Why GP and RDD included?"*

Because, they do use the plausibility and quality report, mainly FSL, to check the data that is sent from missions for validation to HQ.

> *Better way than R templates?*

My original plan was to create one whole system (software) that compile all the processes and be even more user friendly then still running R codes. However, this takes so much time to implement and we didn't have the time. 
So maybe this is something that can be done as a project.

### Knowledge requirements to run/maintain/enhance

- **R:** *Advanced*
- **Git and Github Actions:** *Advanced*
- **Test-Driven Development:** *Advanced*
- **Kobo:** *Intermediate*
- **Python:** *Beginner*

## RLC (Random Location Cluster) Sampling Scripts 

This Github Repo is [LINK](https://github.com/AbrahamAz/rlc_sample) is dedicated for checking the quality of the data collection using the Random Location Cluster sampling or RLC for short as well as producing the weights for the different clusters. 

To understand more about RLC, please check the presentations and the registrations available in the handover folder.

In a nutshell, RLC is a sampling method that requires to define a populated area, and randomly select N amount of GPS locations within this area. Once defined, the 3 nearest neighbouring HH to the defined GPS location are assessed. 

I created a Kobo tool called REACH GEO RAND that allows you to go to the field and trace the areas of interested and the tool will automatically generate N random points within this area with their respective UNIQUE IDs. Please check the presentation of the tool in the handover folder. 

The output is an html report including:

 - *ID MATCHING*: Check Cluster_ID between sampling and rlc data are matching
 - *Clusters inside Area*: Check Clusters are inside the sampling area (50m threshold)
 - *Clusters follow RLC sampling*: Check assessed clusters are random and follow the sampling (50m threshold)
 - *HHs inside sampling area*: Check assessed HHs are inside the sampling are (50m threshold)
 - *Distance HHs to clusters*: Check the distance between the assessed HHs and the sampled cluster points (50m threshold)
 - *Duplicated clusters/HHs*: Check duplication between clusters and check if clusters share same HHs.
 - *Household Density*: Calculate HH Density & Cluster Weights
 - *IDW*: Spatial Interpolation using Inverse Distance Weighing (IDW)
 - *Estimation of Population*: Estimate Population of Sampled Area. (Under Revision)
 
 As well as an Excel file that includes the weights.

### Requirements

This script are built using the REACH GEO RAND output data that consists of:

- **Sheet including the traced areas**
- **Sheet including the randomly generated points**

As well as your assessment data following the RLC sampling methodology that includes:

- **UNIQUE IDs of the randomly generated points**
- **GPS locations of the randomly generated points**
- **GPS locations of the 3 assessed HHs**
- **Number of individuals inside each HH**

### Content

#### **main branch**

##### **data folder**

This folder includes some dummy data for testing the scripts. Both correct data and bad data.

##### **output folder**

This folder includes the outputted HTML & Excel files of both the correct and wrong data. 

##### **resources folder**

This folder includes all the images/logos/local R packages needed to run the script in general. 

##### **src folder**

This folder includes multiple R scripts:

- *init.R*: This script is used to install and load all the packages needed for this scripts. 
- *rlc_quality_check.R* & *rlc_quality_check_fast_track.R*: These are scripts that contains all the functions needed to perform the quality checks and create the outputs.

Also it includes a *utils folder* which includes all the other utility functions needed for cleaning/analysing/reporting. 

##### *main folder*

This folder includes multiple R, Rmd, and batch files. Below is the explanation of each:


- **FAST TRACK**: 
  - *fast_track.R*: I automated the process to be able to run the whole script outside of R. this script is designated to initiate the project and render the Markdown. 
  - *fast_track.bat*: Batch file to run the whole process from outside of R. 
  - *rlc_sample_template_fast_track.Rmd*: The Markdown project that is called inside fast_track.R
  
### Maintenance

There are no maintenance needed for this projects. However, in case of errors, the file to be tracked is the rlc_sample_template_fast_track.Rmd, where you will be able to run each chunk and see where is the error happening. 

### Next Steps Suggestions

One suggestion that I advice for this project is the ability to not only be limited by the REACH GEO RAND as input for the defined area of the study. This will enable to check any RLC sampling methodology for sanity and quality of the data collection.

To change this, it will be required to find a way to provide the assessed households under the RLC Sampling methodology with a unique identifier for each cluster, and be able as well to define an area and input it in the script as a geometry polygon instead of the REACH GEO RAND. 

Most of the function inside src/rlc_quality_checks.R & src/rlc_quality_checks_fast_track.R will be affected by this change, as well as the markdown file. 

# Projects

## Remote Mortality Studies in DRC

### Content of the Repo

This project contains two main folders, the **Cleaning** and the **Analysis** folders. 

The cleaning mainly target the informant method data to prepare it for the Analysis. 

The Analysis folder will be our main focus in this part as we are using it to output all our analysis for the study. 

Below is the full explanation of the folder structure.

#### **data folder**

- *adjust_wall.xlsx*: This is a small excel file created to clean the shelter component for the Network Survival data during the calculations of the socio-economic weighting. 
- *hh_survey_data.xlsx*: This is the HH survey long format data. 
- *Informant_method_full_data_2023_06_01.xlsx*: This is the cleaned informant method data outputed from the Cleaning folder. 
- *mortality_survey_clean_2023-09-16.xlsx*: This is the cleaned HH survey Data normal format (with all other loops included). Mainly used for the Network survival within HH. 
- *network_survival_data_clean.xlsx*: This is the cleaned Network Survival Data. 
- *Population DPS TANG023_ZS_AS_VILLAGE BICRS_revised.xlsx*: This is the Population Data for the ZS/AS used. 

#### **map folder**

- *.csv/.xlsx*: These files are the outputs from the QGIS project to use for the Urban/Rural disaggregations. 
- *rural_urban.qgz*: QGIS Project file. 
- *Catchment_sites and Donnees_DRC_Tanganyika*: These folders include the shapefiles needed for the inputs to the QGIS project. 

#### **output folder**

You will see in this folder 3 main folders:

- *Descriptive*: This folder includes all descriptive analysis (tables/figures/maps) of the 4 conducted mortality studies (HH survey/Informant Studies/Network Survival/Network Survival within HH survey)
- *Evaluation*: This folder includes the comparative analysis done between all the methods.
- *new_analysis*: This folder was created after the email shared by Saeed Rahman for the extra analysis requested. 

<div class='button' data-content="toggle-text"><a href="#8">Here are the set of extra analysis:</a></div>
<div class='abstract'>
<p style="margin-top: -0.1%;">
1)	Effect of Recall Bias (Network Survival and Informant) – Deaths and Births
a.	For each round and method of data collection (HH survey, informant method, kin network, neighbour network remote, neighbour network on survey), please produce a basic table and histograms showing the number of deaths reported per month.
b.	Graphically, please produce 1 overall graph (histogram) per method, facet_wrap by round of reporting, and showing number of deaths reported per month. For the HH survey and Network Survival on HH survey, no need to facet_wrap by round of course.
i.	Casey showed us a similar breakdown, so what I would expect to see is a even distribution of deaths per month on the HH survey method, but a decrease in the early months from the remote methods showing a limitation in reporting. 
c.	Separately do the above also for births reported, though I imagine we can skip this for the informant method where the births were quite unclear. 
d.	Analyze for each remote method at which month the decline in deaths appears prominent, compared to the household survey. Re-produce the crude mortality and birth rates  
</p>
<p style="margin-top: -2.5%;">
2)	Effects of Age of Death on Results (Network Survival and Informant) – It’s possible certain age groups like under-5 children had less We saw a similar graphic from Casey, with 5 year age group bins for crude mortality, which we should try to re-create for our reporting. Ideally we are seeing much higher rates in under-5 and older populations. Please produce basic tables and graphs. 
a.	Graphically, please produce a simple point graph of CDRs with confidence intervals, with the different age groups along the X-axis. Let’s produce the crude mortality for 5-year bins. For each method overall (HH survey, kin network, neighbor network remote, neighbor network on survey). 
b.	Analyze the results – when comparing the CDR by age group across methods, are there where the pattern differs significantly compared to the HH survey results? If so, consider which mortality rates for which age groups is most comparable to the HH survey results. Re-run the overall mortality results for the remote methods excluding the ill-fitting age groups. 
</p>
<p style="margin-top: -2.5%;">
3)	Effect of Rural/Urban on Results (Network Survival and Informant) – For both methods, to understand the effects of rural vs. urban sites on the effectiveness of the methods, for all methods. 
a.	Please first identify and label for all methods (HH survey, network survival, ) which clusters, PSUs, or aire de santes can be considered rural vs. urban – harmonize across methods so we are comparing similar areas. I’d liaise with the DRC team on this, though broadly Kalemie and Nyunzu towns at most should be incorporating the urban areas. 
b.	Produce birth and crude mortality rates overall for rural and urban areas for each of the methods, including HH survey for comparison. 
c.	Produce table of incidence rate ratios and p-values for all comparisons here. 
</p>
<p style="margin-top: -2.5%;">
4)	Effects of Different Kin Relationships on Network Survival Birth and Death Rates (Network Survival Kin Tie Definition) - Specific to Network Survival Kin network for HH survey and remote methods, let’s please produce a basic tables and graphics to understand the distribution of reported deaths and births based on the kin relationship. There should be slightly more kin relationships in the HH survey network survival compared to the remote network survival. What I sort of expect to see here is that the more nebulous relationships like cousins, nieces, nephews, may have lower CDRs and birth rates which could be causing underestimation. 
a.	Tabularly, produce a table with the number of each type of kin reported, total person time for that kin group, and numbers of births, number of crude deaths, and number of under-5 deaths. 
b.	Graphically, for each method please produce a simple point graph of CDRs with confidence intervals by each of the kin relationships. 
c.	Separately, please produce a simple point graph of birth rates with confidence intervals by each of the kin relationships. 
d.	Analyze the results of the above graphs – are there any kin groups that appear to be significantly reporting fewer births or deaths per person-time observed? If so, remove from analysis and re-produce the overall birth and death rates without those groups for comparison. Produce table of incidence rate ratios and p-values for these new comparisons. 
</p>
<p style="margin-top: -2.5%;">
5)	Effects of Type of Key Informant on Informant Method Results (Informant Method) – both by the exact type, but also the broader categories of key informants that we grouped by (male leader, female leader, service provider)
a.	Please produce table summarizing the number of each exact type of KI interviewed, and the total number of households reported on, numbers of births, crude deaths, and under-5 deaths reported by each type. Disaggregate also by ZS, and by Rural/Urban.
b.	Produce a table showing the overlap in number and % in reporting between broad key informant types, consider the format of a 3x3 cross-tabulation with each of the 3 broad types along the horizontal and vertical axes.
c.	Produce a table showing the number of interviews, deaths and person-time reported per round for each broad type of key informant. 
d.	Graphically, produce a Venn Diagram type graphic showing the overlap in number and % in reporting between different broad key informants types. See below an example. 
</p>
<p style="margin-top: -2.5%;">
6)	Capture-Recapture Analysis (Informant Method) – this will help us to determine the “completeness” of reporting of deaths by the informant method and give a proxy idea of the sensitivity of the method. I expect that overall it will be quite poor (50% or less sensitivity) but I suspect that when looking at rural areas alone, it will perform much better (80%+). 
a.	This whole part I think is new to you, so let me know if you want to find some small time in the next few weeks to discuss together and walk through my past scripts. I’m sure there are a few improvements you can make in their efficiency as well. 
b.	Some reference materials for you on capture-recapture and sensitivity
i.	Conceptual and Assumptions of Capture Recapture - http://www.brixtonhealth.com/CRCaseFinding.pdf
ii.	R coding, guidance, and data format requirements - https://hrdag.org/tech-notes/basic-mse.html
iii.	Please also find attached code from my previous implementations of capture-recapture for this method from some trials in SSD. 
c.	Please produce:
i.	Table summarizing unique deaths, estimated ‘uncaptured’ deaths, and estimated sensitivity. Include overall, but also disaggregated by ZS, by rural/urban. 
ii.	Produce and save graphics on the posterior probably estimates from the Bayesian Model Averaging. See an example below. 
</p>

</div>


Inside the new_analysis folder, you will find two folders (graph / table) and inside them different numbered folders depending on the numbered requests.


#### **resource folder**

- *shapefil folder*: This folder include some shapefiles used inside the descriptive analysis folder. 
- *daf_ files.xlsx*: These excel files are dedicated for the some extra indicator analysis for all HH/Informant Study/and Network Survival. The indicators are causes/location of deaths etc. 
- *coverage_MSNA.xlsx*: Just an excel file showing the percentage of MSNA coverage in each Zone de sante
- *death_removed.xlsx*: Excel files showing the death that were removed from the Informant study method after reviewing for duplications between methods. 
- *death.xlsx*: Excel files showing the death that remained from the Informant study method after reviewing for duplications between methods. 
- *.png*: files related for some logos/images.
- *ns_round_weight.xlsx*: Excel file including the weight designed for each 2 month round of the Network Survival. 
- *quick_cleaning_ns.xlsx*: Some extra cleaning for the Network Survival Data
- *quota_sampling_2023-03-01.xlsx & quota_sampling_2023-03-01withtabac.xlsx*: Excel files including the population and the population for each cluster_ID for the Informant Study method (one with Tabac Congo and one without).
- *sampling_frame_.xlsx*: Sampling frames of each method. 
- *tool_.xlsx*: Kobo tool of each method
- *weight_*: Weightings for each method. 

#### **src folder**

- *utils folder*: include all the utility functions used inside the scripts
- *convert_cols_with_daf.R & fix_bugged_names.R*: No need to worry about these scripts (also utility scripts for the extra descriptive analysis).
- *format_dataset_.R*: These scripts is where I am transforming/modelling/calling some extra functions for each method. 
- *init_.R*: These scripts is where I am initiating each method for the analysis. 
- *weight_ses.R*: This script is dedicated for the calculation of the Socio Economic Weight for the Network Survival. 
- *new_analysis.R*: This script is for the new_analysis part asked by Saeed Rahman. 

#### **temp folder**

Only placeholder folder for the extra descriptive analysis. No need to worry about

#### **main folder**

- *run_analysis.R*: This script is to initiate the paths to each excel file and render the Markdown. 
- *analysis_tabular.Rmd*: Markdown where all the analysis is happening. Can be run chunk by chunk, or all together from the run_analysis.R
- *mortalityremote.excalidraw*: If you go to [Excalidraw](https://excalidraw.com/), you will be able to upload this file and look at some of the notes taken for the new_analysis part. 
