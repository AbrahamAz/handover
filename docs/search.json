[
  {
    "objectID": "handover.html",
    "href": "handover.html",
    "title": "PHU Data Specialist Instruction/Guidelines Handbook",
    "section": "",
    "text": "This handbook is dedicated for the next Data Specialist taking over the position within the Public Health Unit in the Global Programmes department at HQ. The book dives into each aspect of the position, showcasing step by step guidances on how to run the existing tools and how to maintain them.\nI will try my best to be as detailed as possible, but do know that other data/technical people will also have answers to all your questions.",
    "crumbs": [
      "PHU Data Specialist Instruction/Guidelines Handbook"
    ]
  },
  {
    "objectID": "handover.html#impactr4phu",
    "href": "handover.html#impactr4phu",
    "title": "PHU Data Specialist Instruction/Guidelines Handbook",
    "section": "impactR4PHU",
    "text": "impactR4PHU\nhealthyR, a tool built by Saeed Rahman (current PHU Unit Manager), has been a crucial tool for checking, cleaning and analysing the data of almost all the outcome indicators of the different sectors of Public Health (Health/Nutrition/Food Security & Livelihoods - FSL/WASH).\nimpactR4PHU followed the same original content structure of heathyR, but addressed some of the feedback from the users, as well as some other bugs/issues. The package also introduced project templates for fast implementations of the processes.\nThis package is following a Test-Driven Development where unit testing, programming, and refactoring of the source code is addapted during development stages.\n\nContent\nHere is a quick dissection of the GitHub Repo, but please do check the READ ME Section for a better description of each component.\n\nmain branch\n\nR Folder\nThis folder includes all the functions that are used by the Public Health Unit for creating, adding, checking, and ploting data within different processes. Almost all functions have a test unit script using the testthat package. The tests can be access through the testing branch. Some of the functions are helpers functions and do not include testers.\nAll functions are built in a way that can be piped within the dplyr system of piping.\nOnce you have checked all the functions, you might ask yourself a question.\n\n“Where are the functions related to Health and WASH?”\n\nThey exist, but not in this package. At Impact, we love to share tasks sometimes, mainly when it is used for specific purposes in other units. You will be able to find these respective functions in the humind package. The humind package is built to address main calculations for indicators collected through the MSNA (Multi-Sectoral Needs Assessment). To know more about this assessment or the package, please reach out to the HPPU Unit colleagues.\nAll functions are well documented using Roxygen, and some funcitons are running some examples using dummy data included in the package.\n\n\ndata Folder\nThis folder includes different dummy datasets used within the examples inside the functions, but also within the test units.\n\n\ninst Folder\nThis folder includes different project templates:\n\nFSL: Food Security and Livelihoods.\nIYCF: Infant Youth Consumption Food TO CHECK\nMortality\nIPHRA: Integrated Public Health Rapid Assessment\nPublic Health integrated tables\n\nYou might ask yourself another question here.\n\nWhy do we have templates if there are already functions?\n\nGood question!! But here is the answer.\nImpact’s field missions are widely different when it come to data resources, capacity, and knowledge. Some missions do not even have data officers. So these templates exists to make the life of users with little knowledge of R easier and faster. They only need to set up R and R Studio on their machines, and run the respective scripts.\nThese project templates can easily be accessed through the “New Project” button in R Studio after installing the Package\n\ndevtools::install_github(“impact-initiatives/impactR4PHU”)\n\nThe scripts interacts with the users through inputs system. Inputs here are very important to make sure that the scripts are outputing the correct intentions.\nPlease make sure to refer back to the respective specialists within the Public Health Unit so they can help you with what to select what when it comes to running the scripts.\n\nMortality: Noortje Gerritzma\nFSL: Olivia Falkowitz\nIYCF: Martin\nIPHRA: PH team\nPH Integrated Tables: PH team or Olivia.\n\nMortality, FSL, and IYCF have 3 templates:\n\nPlausibility and Quality Reports\nCleaning\nDescriptive analysis.\n\nIPHRA have 2 templates:\n\nCleaning\nAnalysis\n\nPH integrated Tables have only 1 template.\nAll the details needed are explained in the READ ME sections of the github page.\n\n\n\ntesting branch\n\ntests Folder\nUsing the testthat package, and followint the TDD method, all functions have been tested following the good and bad path to make sure that they are outputing the expected results. Please do ensure to build tests always with every added functions, and feel free to add tests to existing functions.\n\n\n\n\nMaintenance\nIf changes, improvements, enhancements, add-ons are required to the package, please use the following process to ensure a smooth merging to the main branch.\nFirst, clone the main package to your local machine and create a new branch for your work. If functions are created/ammended, please make sure to checkout to the testing branch and do the necessary work there to keep the package trusted and correctly functioning.\nOnce done, go ahead commit and push your changes to your respective branch, and open a pull requests.\nOnce you create the pull request, some Github Actions will run in teh background to ensure that the package is functional and installable in different operating systems and the coverage test badge is updated. If all the actions pass the tests, I set a requirement here to have a review and validation by another person. However, as a solo data person in the unit, this can be bypassed and merge can be confirmed.\nGithub will again run another test of the Actions and pass a green badge to the R CMD Check.\n\nThe job is not yet done !!\n\nAs all the templates are using there own specific renv environment that also include the impactR4PHU github package, all these renv of the different templates should be updated to the newest version of the package.\nI created a python script that update them automatically instead of accessing them one by one. You will find the link to the python script inside the handover folder in the Sharepoint.\nOnce the script is done, please go again to your local branch where the new changes have been made and you will realise that all renv.lock files have now updated and require a new commit, push and pull for github. I know this is a bit of an annoying process, please go ahead and change it to your convenience.\n\n\nRequirements\n\nPlease inform users to always re-install the package so they can access the latest version, especially if you are constantly enhancing/improving/changing something within the package. This is required because of the renv.locks that includes all newest packages requirements.\nEach template have a renv::update() function in the beginning which is responsible of updating the relevant packages to the latest requirements for the dependencies. However, R sometimes love to be hard and try to break everything. I realized this after receiving some weird “make error” from the field. Kindly keep an eye on that.\n\n\n\nNext Steps Suggestions\nAs the rotation within Impact is relatively high, you should consider having regular refreshers for missions/HQ Global Programs (GP) and Research Department (RDD) on the usage of the package and mainly the templates.\n\n“Why GP and RDD included?”\n\nBecause, they do use the plausibility and quality report, mainly FSL, to check the data that is sent from missions for validation to HQ.\n\nBetter way than R templates?\n\nMy original plan was to create one whole system (software) that compile all the processes and be even more user friendly then still running R codes. However, this takes so much time to implement and we didn’t have the time. So maybe this is something that can be done as a project.\n\n\nKnowledge requirements to run/maintain/enhance\n\nR: Advanced\nGit and Github Actions: Advanced\nTest-Driven Development: Advanced\nKobo: Intermediate\nPython: Beginner",
    "crumbs": [
      "PHU Data Specialist Instruction/Guidelines Handbook"
    ]
  },
  {
    "objectID": "handover.html#rlc-random-location-cluster-sampling-scripts",
    "href": "handover.html#rlc-random-location-cluster-sampling-scripts",
    "title": "PHU Data Specialist Instruction/Guidelines Handbook",
    "section": "RLC (Random Location Cluster) Sampling Scripts",
    "text": "RLC (Random Location Cluster) Sampling Scripts\nThis Github Repo is LINK is dedicated for checking the quality of the data collection using the Random Location Cluster sampling or RLC for short as well as producing the weights for the different clusters.\nTo understand more about RLC, please check the presentations and the registrations available in the handover folder.\nIn a nutshell, RLC is a sampling method that requires to define a populated area, and randomly select N amount of GPS locations within this area. Once defined, the 3 nearest neighbouring HH to the defined GPS location are assessed.\nI created a Kobo tool called REACH GEO RAND that allows you to go to the field and trace the areas of interested and the tool will automatically generate N random points within this area with their respective UNIQUE IDs. Please check the presentation of the tool in the handover folder.\nThe output is an html report including:\n\nID MATCHING: Check Cluster_ID between sampling and rlc data are matching\nClusters inside Area: Check Clusters are inside the sampling area (50m threshold)\nClusters follow RLC sampling: Check assessed clusters are random and follow the sampling (50m threshold)\nHHs inside sampling area: Check assessed HHs are inside the sampling are (50m threshold)\nDistance HHs to clusters: Check the distance between the assessed HHs and the sampled cluster points (50m threshold)\nDuplicated clusters/HHs: Check duplication between clusters and check if clusters share same HHs.\nHousehold Density: Calculate HH Density & Cluster Weights\nIDW: Spatial Interpolation using Inverse Distance Weighing (IDW)\nEstimation of Population: Estimate Population of Sampled Area. (Under Revision)\n\nAs well as an Excel file that includes the weights.\n\nRequirements\nThis script are built using the REACH GEO RAND output data that consists of:\n\nSheet including the traced areas\nSheet including the randomly generated points\n\nAs well as your assessment data following the RLC sampling methodology that includes:\n\nUNIQUE IDs of the randomly generated points\nGPS locations of the randomly generated points\nGPS locations of the 3 assessed HHs\nNumber of individuals inside each HH\n\n\n\nContent\n\nmain branch\n\ndata folder\nThis folder includes some dummy data for testing the scripts. Both correct data and bad data.\n\n\noutput folder\nThis folder includes the outputted HTML & Excel files of both the correct and wrong data.\n\n\nresources folder\nThis folder includes all the images/logos/local R packages needed to run the script in general.\n\n\nsrc folder\nThis folder includes multiple R scripts:\n\ninit.R: This script is used to install and load all the packages needed for this scripts.\nrlc_quality_check.R & rlc_quality_check_fast_track.R: These are scripts that contains all the functions needed to perform the quality checks and create the outputs.\n\nAlso it includes a utils folder which includes all the other utility functions needed for cleaning/analysing/reporting.\n\n\nmain folder\nThis folder includes multiple R, Rmd, and batch files. Below is the explanation of each:\n\nFAST TRACK:\n\nfast_track.R: I automated the process to be able to run the whole script outside of R. this script is designated to initiate the project and render the Markdown.\nfast_track.bat: Batch file to run the whole process from outside of R.\nrlc_sample_template_fast_track.Rmd: The Markdown project that is called inside fast_track.R\n\n\n\n\n\n\nMaintenance\nThere are no maintenance needed for this projects. However, in case of errors, the file to be tracked is the rlc_sample_template_fast_track.Rmd, where you will be able to run each chunk and see where is the error happening.\n\n\nNext Steps Suggestions\nOne suggestion that I advice for this project is the ability to not only be limited by the REACH GEO RAND as input for the defined area of the study. This will enable to check any RLC sampling methodology for sanity and quality of the data collection.\nTo change this, it will be required to find a way to provide the assessed households under the RLC Sampling methodology with a unique identifier for each cluster, and be able as well to define an area and input it in the script as a geometry polygon instead of the REACH GEO RAND.\nMost of the function inside src/rlc_quality_checks.R & src/rlc_quality_checks_fast_track.R will be affected by this change, as well as the markdown file.",
    "crumbs": [
      "PHU Data Specialist Instruction/Guidelines Handbook"
    ]
  },
  {
    "objectID": "handover.html#remote-mortality-studies-in-drc",
    "href": "handover.html#remote-mortality-studies-in-drc",
    "title": "PHU Data Specialist Instruction/Guidelines Handbook",
    "section": "Remote Mortality Studies in DRC",
    "text": "Remote Mortality Studies in DRC\n\nContent of the Repo\nThis project contains two main folders, the Cleaning and the Analysis folders.\nThe cleaning mainly target the informant method data to prepare it for the Analysis.\nThe Analysis folder will be our main focus in this part as we are using it to output all our analysis for the study.\nBelow is the full explanation of the folder structure.\n\ndata folder\n\nadjust_wall.xlsx: This is a small excel file created to clean the shelter component for the Network Survival data during the calculations of the socio-economic weighting.\nhh_survey_data.xlsx: This is the HH survey long format data.\nInformant_method_full_data_2023_06_01.xlsx: This is the cleaned informant method data outputed from the Cleaning folder.\nmortality_survey_clean_2023-09-16.xlsx: This is the cleaned HH survey Data normal format (with all other loops included). Mainly used for the Network survival within HH.\nnetwork_survival_data_clean.xlsx: This is the cleaned Network Survival Data.\nPopulation DPS TANG023_ZS_AS_VILLAGE BICRS_revised.xlsx: This is the Population Data for the ZS/AS used.\n\n\n\nmap folder\n\n.csv/.xlsx: These files are the outputs from the QGIS project to use for the Urban/Rural disaggregations.\nrural_urban.qgz: QGIS Project file.\nCatchment_sites and Donnees_DRC_Tanganyika: These folders include the shapefiles needed for the inputs to the QGIS project.\n\n\n\noutput folder\nYou will see in this folder 3 main folders:\n\nDescriptive: This folder includes all descriptive analysis (tables/figures/maps) of the 4 conducted mortality studies (HH survey/Informant Studies/Network Survival/Network Survival within HH survey)\nEvaluation: This folder includes the comparative analysis done between all the methods.\nnew_analysis: This folder was created after the email shared by Saeed Rahman for the extra analysis requested.\n\n\nHere are the set of extra analysis:\n\n\n\n\nEffect of Recall Bias (Network Survival and Informant) – Deaths and Births\n\n\nFor each round and method of data collection (HH survey, informant method, kin network, neighbour network remote, neighbour network on survey), please produce a basic table and histograms showing the number of deaths reported per month.\nGraphically, please produce 1 overall graph (histogram) per method, facet_wrap by round of reporting, and showing number of deaths reported per month. For the HH survey and Network Survival on HH survey, no need to facet_wrap by round of course.\nCasey showed us a similar breakdown, so what I would expect to see is a even distribution of deaths per month on the HH survey method, but a decrease in the early months from the remote methods showing a limitation in reporting.\nSeparately do the above also for births reported, though I imagine we can skip this for the informant method where the births were quite unclear.\nAnalyze for each remote method at which month the decline in deaths appears prominent, compared to the household survey. Re-produce the crude mortality and birth rates\n\n\n\n\n\nEffects of Age of Death on Results (Network Survival and Informant) – It’s possible certain age groups like under-5 children had less We saw a similar graphic from Casey, with 5 year age group bins for crude mortality, which we should try to re-create for our reporting. Ideally we are seeing much higher rates in under-5 and older populations. Please produce basic tables and graphs.\n\n\nGraphically, please produce a simple point graph of CDRs with confidence intervals, with the different age groups along the X-axis. Let’s produce the crude mortality for 5-year bins. For each method overall (HH survey, kin network, neighbor network remote, neighbor network on survey).\nAnalyze the results – when comparing the CDR by age group across methods, are there where the pattern differs significantly compared to the HH survey results? If so, consider which mortality rates for which age groups is most comparable to the HH survey results. Re-run the overall mortality results for the remote methods excluding the ill-fitting age groups.\n\n\n\n\nEffect of Rural/Urban on Results (Network Survival and Informant) – For both methods, to understand the effects of rural vs. urban sites on the effectiveness of the methods, for all methods.\n\n\nPlease first identify and label for all methods (HH survey, network survival, ) which clusters, PSUs, or aire de santes can be considered rural vs. urban – harmonize across methods so we are comparing similar areas. I’d liaise with the DRC team on this, though broadly Kalemie and Nyunzu towns at most should be incorporating the urban areas.\nProduce birth and crude mortality rates overall for rural and urban areas for each of the methods, including HH survey for comparison.\nProduce table of incidence rate ratios and p-values for all comparisons here.\n\n\n\n\nEffects of Different Kin Relationships on Network Survival Birth and Death Rates (Network Survival Kin Tie Definition) - Specific to Network Survival Kin network for HH survey and remote methods, let’s please produce a basic tables and graphics to understand the distribution of reported deaths and births based on the kin relationship. There should be slightly more kin relationships in the HH survey network survival compared to the remote network survival. What I sort of expect to see here is that the more nebulous relationships like cousins, nieces, nephews, may have lower CDRs and birth rates which could be causing underestimation.\n\n\nTabularly, produce a table with the number of each type of kin reported, total person time for that kin group, and numbers of births, number of crude deaths, and number of under-5 deaths.\nGraphically, for each method please produce a simple point graph of CDRs with confidence intervals by each of the kin relationships.\nSeparately, please produce a simple point graph of birth rates with confidence intervals by each of the kin relationships.\nAnalyze the results of the above graphs – are there any kin groups that appear to be significantly reporting fewer births or deaths per person-time observed? If so, remove from analysis and re-produce the overall birth and death rates without those groups for comparison. Produce table of incidence rate ratios and p-values for these new comparisons.\n\n\n\n\nEffects of Type of Key Informant on Informant Method Results (Informant Method) – both by the exact type, but also the broader categories of key informants that we grouped by (male leader, female leader, service provider)\n\n\nPlease produce table summarizing the number of each exact type of KI interviewed, and the total number of households reported on, numbers of births, crude deaths, and under-5 deaths reported by each type. Disaggregate also by ZS, and by Rural/Urban.\nProduce a table showing the overlap in number and % in reporting between broad key informant types, consider the format of a 3x3 cross-tabulation with each of the 3 broad types along the horizontal and vertical axes.\nProduce a table showing the number of interviews, deaths and person-time reported per round for each broad type of key informant.\nGraphically, produce a Venn Diagram type graphic showing the overlap in number and % in reporting between different broad key informants types. See below an example.\n\n\n\n\nCapture-Recapture Analysis (Informant Method) – this will help us to determine the “completeness” of reporting of deaths by the informant method and give a proxy idea of the sensitivity of the method. I expect that overall it will be quite poor (50% or less sensitivity) but I suspect that when looking at rural areas alone, it will perform much better (80%+).\n\n\nThis whole part I think is new to you, so let me know if you want to find some small time in the next few weeks to discuss together and walk through my past scripts. I’m sure there are a few improvements you can make in their efficiency as well.\nSome reference materials for you on capture-recapture and sensitivity\nConceptual and Assumptions of Capture Recapture - http://www.brixtonhealth.com/CRCaseFinding.pdf\n\n\nR coding, guidance, and data format requirements - https://hrdag.org/tech-notes/basic-mse.html\nPlease also find attached code from my previous implementations of capture-recapture for this method from some trials in SSD.\nPlease produce:\nTable summarizing unique deaths, estimated ‘uncaptured’ deaths, and estimated sensitivity. Include overall, but also disaggregated by ZS, by rural/urban.\nProduce and save graphics on the posterior probably estimates from the Bayesian Model Averaging. See an example below.\n\n\n\nInside the new_analysis folder, you will find two folders (graph / table) and inside them different numbered folders depending on the numbered requests.\n\n\nresource folder\n\nshapefil folder: This folder include some shapefiles used inside the descriptive analysis folder.\ndaf_ files.xlsx: These excel files are dedicated for the some extra indicator analysis for all HH/Informant Study/and Network Survival. The indicators are causes/location of deaths etc.\ncoverage_MSNA.xlsx: Just an excel file showing the percentage of MSNA coverage in each Zone de sante\ndeath_removed.xlsx: Excel files showing the death that were removed from the Informant study method after reviewing for duplications between methods.\ndeath.xlsx: Excel files showing the death that remained from the Informant study method after reviewing for duplications between methods.\n.png: files related for some logos/images.\nns_round_weight.xlsx: Excel file including the weight designed for each 2 month round of the Network Survival.\nquick_cleaning_ns.xlsx: Some extra cleaning for the Network Survival Data\nquota_sampling_2023-03-01.xlsx & quota_sampling_2023-03-01withtabac.xlsx: Excel files including the population and the population for each cluster_ID for the Informant Study method (one with Tabac Congo and one without).\nsampling_frame_.xlsx: Sampling frames of each method.\ntool_.xlsx: Kobo tool of each method\nweight_: Weightings for each method.\n\n\n\nsrc folder\n\nutils folder: include all the utility functions used inside the scripts\nconvert_cols_with_daf.R & fix_bugged_names.R: No need to worry about these scripts (also utility scripts for the extra descriptive analysis).\nformat_dataset_.R: These scripts is where I am transforming/modelling/calling some extra functions for each method.\ninit_.R: These scripts is where I am initiating each method for the analysis.\nweight_ses.R: This script is dedicated for the calculation of the Socio Economic Weight for the Network Survival.\nnew_analysis.R: This script is for the new_analysis part asked by Saeed Rahman.\n\n\n\ntemp folder\nOnly placeholder folder for the extra descriptive analysis. No need to worry about\n\n\nmain folder\n\nrun_analysis.R: This script is to initiate the paths to each excel file and render the Markdown.\nanalysis_tabular.Rmd: Markdown where all the analysis is happening. Can be run chunk by chunk, or all together from the run_analysis.R\nmortalityremote.excalidraw: If you go to Excalidraw, you will be able to upload this file and look at some of the notes taken for the new_analysis part.",
    "crumbs": [
      "PHU Data Specialist Instruction/Guidelines Handbook"
    ]
  }
]