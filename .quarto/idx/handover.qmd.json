{"title":"PHU Data Specialist Instruction/Guidelines Handbook","markdown":{"yaml":{"title":"PHU Data Specialist Instruction/Guidelines Handbook","author":"Abraham Azar","date":"last-modified"},"headingText":"Introduction","containsRefs":false,"markdown":"\n<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n<script>\n  $(document).ready(function () {\n    $(\".abstract\").hide();\n    $(\".button\").on(\"click\", function () {\n        $(this).next(\".abstract\").slideToggle(400);\n    });\n});\n</script>\n\n<style>\n.abstract{text-align:justify; }\n.button{ text-align:justify; }\n</style>\n\n\n\nThis handbook is dedicated for the next Data Specialist taking over the position within the Public Health Unit in the Global Programmes department at HQ. The book dives into each aspect of the position, showcasing step by step guidances on how to run the existing tools and how to maintain them. \n\nI will try my best to be as detailed as possible, but do know that other data/technical people will also have answers to all your questions.\n\n# Tools\n\n## impactR4PHU\n\n[healthyR](https://github.com/SaeedR1987/healthyr), a tool built by Saeed Rahman (current PHU Unit Manager), has been a crucial tool for checking, cleaning and analysing the data of almost all the outcome indicators of the different sectors of Public Health (Health/Nutrition/Food Security & Livelihoods - FSL/WASH). \n\n[impactR4PHU](https://github.com/impact-initiatives/impactR4PHU) followed the same original content structure of heathyR, but addressed some of the feedback from the users, as well as some other bugs/issues. The package also introduced project templates for fast implementations of the processes. \n\nThis package is following a [Test-Driven Development](https://www.techtarget.com/searchsoftwarequality/definition/test-driven-development) where unit testing, programming, and refactoring of the source code is addapted during development stages. \n\n### Content\n\nHere is a quick dissection of the GitHub Repo, but please do check the READ ME Section for a better description of each component. \n\n#### **main branch**\n\n##### **R Folder**\n\nThis folder includes all the functions that are used by the Public Health Unit for *creating*, *adding*, *checking*, and *ploting* data within different processes. Almost all functions have a test unit script using the [testthat](https://testthat.r-lib.org/) package. The tests can be access through the testing branch. Some of the functions are helpers functions and do not include testers. \n    \nAll functions are built in a way that can be piped within the **dplyr** system of piping.\n\nOnce you have checked all the functions, you might ask yourself a question. \n\n> *\"Where are the functions related to Health and WASH?\"* \n\nThey exist, but not in this package. At Impact, we love to share tasks sometimes, mainly when it is used for specific purposes in other units. You will be able to find these respective functions in the [humind](https://github.com/impact-initiatives-hppu/humind) package. The humind package is built to address main calculations for indicators collected through the MSNA (Multi-Sectoral Needs Assessment). To know more about this assessment or the package, please reach out to the HPPU Unit colleagues.\n\nAll functions are well documented using Roxygen, and some funcitons are running some examples using dummy data included in the package. \n\n##### **data Folder**\n\nThis folder includes different dummy datasets used within the examples inside the functions, but also within the test units. \n\n##### **inst Folder**\n\nThis folder includes different project templates:\n\n- FSL: Food Security and Livelihoods.\n- IYCF: Infant Youth Consumption Food TO CHECK\n- Mortality\n- IPHRA: Integrated Public Health Rapid Assessment\n- Public Health integrated tables\n\nYou might ask yourself another question here.\n\n> *Why do we have templates if there are already functions?*\n\nGood question!! But here is the answer.\n\nImpact's field missions are widely different when it come to data resources, capacity, and knowledge. Some missions do not even have data officers. So these templates exists to make the life of users with little knowledge of R easier and faster. They only need to set up R and R Studio on their machines, and run the respective scripts. \n\nThese project templates can easily be accessed through the \"New Project\" button in R Studio after installing the Package\n\n> devtools::install_github(\"impact-initiatives/impactR4PHU\")\n\nThe scripts interacts with the users through inputs system. Inputs here are very important to make sure that the scripts are outputing the correct intentions.\n\nPlease make sure to refer back to the respective specialists within the Public Health Unit so they can help you with what to select what when it comes to running the scripts.\n\n- Mortality: Noortje Gerritzma\n- FSL: Olivia Falkowitz\n- IYCF: Martin \n- IPHRA: PH team\n- PH Integrated Tables: PH team or Olivia. \n\nMortality, FSL, and IYCF have 3 templates:\n\n- Plausibility and Quality Reports\n- Cleaning\n- Descriptive analysis.\n\nIPHRA have 2 templates:\n\n- Cleaning\n- Analysis\n\nPH integrated Tables have only 1 template. \n\nAll the details needed are explained in the READ ME sections of the github page. \n\n#### **testing branch**\n\n##### **tests Folder** \n\nUsing the testthat package, and followint the TDD method, all functions have been tested following the good and bad path to make sure that they are outputing the expected results. Please do ensure to build tests always with every added functions, and feel free to add tests to existing functions. \n\n### Maintenance\n\nIf changes, improvements, enhancements, add-ons are required to the package, please use the following process to ensure a smooth merging to the main branch. \n\nFirst, clone the main package to your local machine and create a new branch for your work. \nIf functions are created/ammended, **please** make sure to checkout to the testing branch and do the necessary work there to keep the package trusted and correctly functioning. \n\nOnce done, go ahead commit and push your changes to your respective branch, and open a pull requests. \n\nOnce you create the pull request, some Github Actions will run in teh background to ensure that the package is functional and installable in different operating systems and the coverage test badge is updated. If all the actions pass the tests, I set a requirement here to have a review and validation by another person. However, as a solo data person in the unit, this can be bypassed and merge can be confirmed. \n\nGithub will again run another test of the Actions and pass a green badge to the R CMD Check. \n\n> The job is not yet done !!\n\nAs all the templates are using there own specific renv environment that also include the impactR4PHU github package, all these renv of the different templates should be updated to the newest version of the package. \n\nI created a python script that update them automatically instead of accessing them one by one. You will find the link to the python script inside the handover folder in the Sharepoint. \n\nOnce the script is done, please go again to your local branch where the new changes have been made and you will realise that all renv.lock files have now updated and require a new commit, push and pull for github. I know this is a bit of an annoying process, please go ahead and change it to your convenience. \n\n### Requirements\n\n- Please inform users to always re-install the package so they can access the latest version, especially if you are constantly enhancing/improving/changing something within the package. This is required because of the renv.locks that includes all newest packages requirements. \n\n- Each template have a renv::update() function in the beginning which is responsible of updating the relevant packages to the latest requirements for the dependencies. However, R sometimes love to be hard and try to break everything. I realized this after receiving some weird **\"make error\"** from the field. Kindly keep an eye on that. \n\n- Create a FAQ section compiled from the field.\n\n### Next Steps Suggestions\n\nAs the rotation within Impact is relatively high, you should consider having regular refreshers for missions/HQ Global Programs (GP) and Research Department (RDD) on the usage of the package and mainly the templates. \n\n> *\"Why GP and RDD included?\"*\n\nBecause, they do use the plausibility and quality report, mainly FSL, to check the data that is sent from missions for validation to HQ.\n\n> *Better way than R templates?*\n\nMy original plan was to create one whole system (software) that compile all the processes and be even more user friendly then still running R codes. However, this takes so much time to implement and we didn't have the time. \nSo maybe this is something that can be done as a project.\n\n### Knowledge requirements to run/maintain/enhance\n\n- **R:** *Advanced*\n- **Git and Github Actions:** *Advanced*\n- **Test-Driven Development:** *Advanced*\n- **Kobo:** *Intermediate*\n- **Python:** *Beginner*\n\n## RLC (Random Location Cluster) Sampling Scripts \n\nThis Github Repo is [LINK](https://github.com/AbrahamAz/rlc_sample) is dedicated for checking the quality of the data collection using the Random Location Cluster sampling or RLC for short as well as producing the weights for the different clusters. \n\nTo understand more about RLC, please check the presentations and the registrations available in the handover folder.\n\nIn a nutshell, RLC is a sampling method that requires to define a populated area, and randomly select N amount of GPS locations within this area. Once defined, the 3 nearest neighbouring HH to the defined GPS location are assessed. \n\nI created a Kobo tool called REACH GEO RAND that allows you to go to the field and trace the areas of interested and the tool will automatically generate N random points within this area with their respective UNIQUE IDs. Please check the presentation of the tool in the handover folder. \n\nThe output is an html report including:\n\n - *ID MATCHING*: Check Cluster_ID between sampling and rlc data are matching\n - *Clusters inside Area*: Check Clusters are inside the sampling area (50m threshold)\n - *Clusters follow RLC sampling*: Check assessed clusters are random and follow the sampling (50m threshold)\n - *HHs inside sampling area*: Check assessed HHs are inside the sampling are (50m threshold)\n - *Distance HHs to clusters*: Check the distance between the assessed HHs and the sampled cluster points (50m threshold)\n - *Duplicated clusters/HHs*: Check duplication between clusters and check if clusters share same HHs.\n - *Household Density*: Calculate HH Density & Cluster Weights\n - *IDW*: Spatial Interpolation using Inverse Distance Weighing (IDW)\n - *Estimation of Population*: Estimate Population of Sampled Area. (Under Revision)\n \n As well as an Excel file that includes the weights.\n\n### Requirements\n\nThis script are built using the REACH GEO RAND output data that consists of:\n\n- **Sheet including the traced areas**\n- **Sheet including the randomly generated points**\n\nAs well as your assessment data following the RLC sampling methodology that includes:\n\n- **UNIQUE IDs of the randomly generated points**\n- **GPS locations of the randomly generated points**\n- **GPS locations of the 3 assessed HHs**\n- **Number of individuals inside each HH**\n\n### Content\n\n#### **main branch**\n\n##### **data folder**\n\nThis folder includes some dummy data for testing the scripts. Both correct data and bad data.\n\n##### **output folder**\n\nThis folder includes the outputted HTML & Excel files of both the correct and wrong data. \n\n##### **resources folder**\n\nThis folder includes all the images/logos/local R packages needed to run the script in general. \n\n##### **src folder**\n\nThis folder includes multiple R scripts:\n\n- *init.R*: This script is used to install and load all the packages needed for this scripts. \n- *rlc_quality_check.R* & *rlc_quality_check_fast_track.R*: These are scripts that contains all the functions needed to perform the quality checks and create the outputs.\n\nAlso it includes a *utils folder* which includes all the other utility functions needed for cleaning/analysing/reporting. \n\n##### *main folder*\n\nThis folder includes multiple R, Rmd, and batch files. Below is the explanation of each:\n\n\n- **FAST TRACK**: \n  - *fast_track.R*: I automated the process to be able to run the whole script outside of R. this script is designated to initiate the project and render the Markdown. \n  - *fast_track.bat*: Batch file to run the whole process from outside of R. \n  - *rlc_sample_template_fast_track.Rmd*: The Markdown project that is called inside fast_track.R\n  \n### Maintenance\n\nThere are no maintenance needed for this projects. However, in case of errors, the file to be tracked is the rlc_sample_template_fast_track.Rmd, where you will be able to run each chunk and see where is the error happening. \n\n### Next Steps Suggestions\n\nOne suggestion that I advice for this project is the ability to not only be limited by the REACH GEO RAND as input for the defined area of the study. This will enable to check any RLC sampling methodology for sanity and quality of the data collection.\n\nTo change this, it will be required to find a way to provide the assessed households under the RLC Sampling methodology with a unique identifier for each cluster, and be able as well to define an area and input it in the script as a geometry polygon instead of the REACH GEO RAND. \n\nMost of the function inside src/rlc_quality_checks.R & src/rlc_quality_checks_fast_track.R will be affected by this change, as well as the markdown file. \n\n# Projects\n\n## Remote Mortality Studies in DRC\n\n### Content of the Repo\n\nThis project contains two main folders, the **Cleaning** and the **Analysis** folders. \n\nThe cleaning mainly target the informant method data to prepare it for the Analysis. \n\nThe Analysis folder will be our main focus in this part as we are using it to output all our analysis for the study. \n\nBelow is the full explanation of the folder structure.\n\n#### **data folder**\n\n- *adjust_wall.xlsx*: This is a small excel file created to clean the shelter component for the Network Survival data during the calculations of the socio-economic weighting. \n- *hh_survey_data.xlsx*: This is the HH survey long format data. \n- *Informant_method_full_data_2023_06_01.xlsx*: This is the cleaned informant method data outputed from the Cleaning folder. \n- *mortality_survey_clean_2023-09-16.xlsx*: This is the cleaned HH survey Data normal format (with all other loops included). Mainly used for the Network survival within HH. \n- *network_survival_data_clean.xlsx*: This is the cleaned Network Survival Data. \n- *Population DPS TANG023_ZS_AS_VILLAGE BICRS_revised.xlsx*: This is the Population Data for the ZS/AS used. \n\n#### **map folder**\n\n- *.csv/.xlsx*: These files are the outputs from the QGIS project to use for the Urban/Rural disaggregations. \n- *rural_urban.qgz*: QGIS Project file. \n- *Catchment_sites and Donnees_DRC_Tanganyika*: These folders include the shapefiles needed for the inputs to the QGIS project. \n\n#### **output folder**\n\nYou will see in this folder 3 main folders:\n\n- *Descriptive*: This folder includes all descriptive analysis (tables/figures/maps) of the 4 conducted mortality studies (HH survey/Informant Studies/Network Survival/Network Survival within HH survey)\n- *Evaluation*: This folder includes the comparative analysis done between all the methods.\n- *new_analysis*: This folder was created after the email shared by Saeed Rahman for the extra analysis requested. \n\n<div class='button' data-content=\"toggle-text\"><a href=\"#8\">Here are the set of extra analysis:</a></div>\n<div class='abstract'>\n<p style=\"margin-top: -0.1%;\">\n1)\tEffect of Recall Bias (Network Survival and Informant) – Deaths and Births\na.\tFor each round and method of data collection (HH survey, informant method, kin network, neighbour network remote, neighbour network on survey), please produce a basic table and histograms showing the number of deaths reported per month.\nb.\tGraphically, please produce 1 overall graph (histogram) per method, facet_wrap by round of reporting, and showing number of deaths reported per month. For the HH survey and Network Survival on HH survey, no need to facet_wrap by round of course.\ni.\tCasey showed us a similar breakdown, so what I would expect to see is a even distribution of deaths per month on the HH survey method, but a decrease in the early months from the remote methods showing a limitation in reporting. \nc.\tSeparately do the above also for births reported, though I imagine we can skip this for the informant method where the births were quite unclear. \nd.\tAnalyze for each remote method at which month the decline in deaths appears prominent, compared to the household survey. Re-produce the crude mortality and birth rates  \n</p>\n<p style=\"margin-top: -2.5%;\">\n2)\tEffects of Age of Death on Results (Network Survival and Informant) – It’s possible certain age groups like under-5 children had less We saw a similar graphic from Casey, with 5 year age group bins for crude mortality, which we should try to re-create for our reporting. Ideally we are seeing much higher rates in under-5 and older populations. Please produce basic tables and graphs. \na.\tGraphically, please produce a simple point graph of CDRs with confidence intervals, with the different age groups along the X-axis. Let’s produce the crude mortality for 5-year bins. For each method overall (HH survey, kin network, neighbor network remote, neighbor network on survey). \nb.\tAnalyze the results – when comparing the CDR by age group across methods, are there where the pattern differs significantly compared to the HH survey results? If so, consider which mortality rates for which age groups is most comparable to the HH survey results. Re-run the overall mortality results for the remote methods excluding the ill-fitting age groups. \n</p>\n<p style=\"margin-top: -2.5%;\">\n3)\tEffect of Rural/Urban on Results (Network Survival and Informant) – For both methods, to understand the effects of rural vs. urban sites on the effectiveness of the methods, for all methods. \na.\tPlease first identify and label for all methods (HH survey, network survival, ) which clusters, PSUs, or aire de santes can be considered rural vs. urban – harmonize across methods so we are comparing similar areas. I’d liaise with the DRC team on this, though broadly Kalemie and Nyunzu towns at most should be incorporating the urban areas. \nb.\tProduce birth and crude mortality rates overall for rural and urban areas for each of the methods, including HH survey for comparison. \nc.\tProduce table of incidence rate ratios and p-values for all comparisons here. \n</p>\n<p style=\"margin-top: -2.5%;\">\n4)\tEffects of Different Kin Relationships on Network Survival Birth and Death Rates (Network Survival Kin Tie Definition) - Specific to Network Survival Kin network for HH survey and remote methods, let’s please produce a basic tables and graphics to understand the distribution of reported deaths and births based on the kin relationship. There should be slightly more kin relationships in the HH survey network survival compared to the remote network survival. What I sort of expect to see here is that the more nebulous relationships like cousins, nieces, nephews, may have lower CDRs and birth rates which could be causing underestimation. \na.\tTabularly, produce a table with the number of each type of kin reported, total person time for that kin group, and numbers of births, number of crude deaths, and number of under-5 deaths. \nb.\tGraphically, for each method please produce a simple point graph of CDRs with confidence intervals by each of the kin relationships. \nc.\tSeparately, please produce a simple point graph of birth rates with confidence intervals by each of the kin relationships. \nd.\tAnalyze the results of the above graphs – are there any kin groups that appear to be significantly reporting fewer births or deaths per person-time observed? If so, remove from analysis and re-produce the overall birth and death rates without those groups for comparison. Produce table of incidence rate ratios and p-values for these new comparisons. \n</p>\n<p style=\"margin-top: -2.5%;\">\n5)\tEffects of Type of Key Informant on Informant Method Results (Informant Method) – both by the exact type, but also the broader categories of key informants that we grouped by (male leader, female leader, service provider)\na.\tPlease produce table summarizing the number of each exact type of KI interviewed, and the total number of households reported on, numbers of births, crude deaths, and under-5 deaths reported by each type. Disaggregate also by ZS, and by Rural/Urban.\nb.\tProduce a table showing the overlap in number and % in reporting between broad key informant types, consider the format of a 3x3 cross-tabulation with each of the 3 broad types along the horizontal and vertical axes.\nc.\tProduce a table showing the number of interviews, deaths and person-time reported per round for each broad type of key informant. \nd.\tGraphically, produce a Venn Diagram type graphic showing the overlap in number and % in reporting between different broad key informants types. See below an example. \n</p>\n<p style=\"margin-top: -2.5%;\">\n6)\tCapture-Recapture Analysis (Informant Method) – this will help us to determine the “completeness” of reporting of deaths by the informant method and give a proxy idea of the sensitivity of the method. I expect that overall it will be quite poor (50% or less sensitivity) but I suspect that when looking at rural areas alone, it will perform much better (80%+). \na.\tThis whole part I think is new to you, so let me know if you want to find some small time in the next few weeks to discuss together and walk through my past scripts. I’m sure there are a few improvements you can make in their efficiency as well. \nb.\tSome reference materials for you on capture-recapture and sensitivity\ni.\tConceptual and Assumptions of Capture Recapture - http://www.brixtonhealth.com/CRCaseFinding.pdf\nii.\tR coding, guidance, and data format requirements - https://hrdag.org/tech-notes/basic-mse.html\niii.\tPlease also find attached code from my previous implementations of capture-recapture for this method from some trials in SSD. \nc.\tPlease produce:\ni.\tTable summarizing unique deaths, estimated ‘uncaptured’ deaths, and estimated sensitivity. Include overall, but also disaggregated by ZS, by rural/urban. \nii.\tProduce and save graphics on the posterior probably estimates from the Bayesian Model Averaging. See an example below. \n</p>\n\n</div>\n\n\nInside the new_analysis folder, you will find two folders (graph / table) and inside them different numbered folders depending on the numbered requests.\n\n\n#### **resource folder**\n\n- *shapefil folder*: This folder include some shapefiles used inside the descriptive analysis folder. \n- *daf_ files.xlsx*: These excel files are dedicated for the some extra indicator analysis for all HH/Informant Study/and Network Survival. The indicators are causes/location of deaths etc. \n- *coverage_MSNA.xlsx*: Just an excel file showing the percentage of MSNA coverage in each Zone de sante\n- *death_removed.xlsx*: Excel files showing the death that were removed from the Informant study method after reviewing for duplications between methods. \n- *death.xlsx*: Excel files showing the death that remained from the Informant study method after reviewing for duplications between methods. \n- *.png*: files related for some logos/images.\n- *ns_round_weight.xlsx*: Excel file including the weight designed for each 2 month round of the Network Survival. \n- *quick_cleaning_ns.xlsx*: Some extra cleaning for the Network Survival Data\n- *quota_sampling_2023-03-01.xlsx & quota_sampling_2023-03-01withtabac.xlsx*: Excel files including the population and the population for each cluster_ID for the Informant Study method (one with Tabac Congo and one without).\n- *sampling_frame_.xlsx*: Sampling frames of each method. \n- *tool_.xlsx*: Kobo tool of each method\n- *weight_*: Weightings for each method. \n\n#### **src folder**\n\n- *utils folder*: include all the utility functions used inside the scripts\n- *convert_cols_with_daf.R & fix_bugged_names.R*: No need to worry about these scripts (also utility scripts for the extra descriptive analysis).\n- *format_dataset_.R*: These scripts is where I am transforming/modelling/calling some extra functions for each method. \n- *init_.R*: These scripts is where I am initiating each method for the analysis. \n- *weight_ses.R*: This script is dedicated for the calculation of the Socio Economic Weight for the Network Survival. \n- *new_analysis.R*: This script is for the new_analysis part asked by Saeed Rahman. \n\n#### **temp folder**\n\nOnly placeholder folder for the extra descriptive analysis. No need to worry about\n\n#### **main folder**\n\n- *run_analysis.R*: This script is to initiate the paths to each excel file and render the Markdown. \n- *analysis_tabular.Rmd*: Markdown where all the analysis is happening. Can be run chunk by chunk, or all together from the run_analysis.R\n- *mortalityremote.excalidraw*: If you go to [Excalidraw](https://excalidraw.com/), you will be able to upload this file and look at some of the notes taken for the new_analysis part. \n","srcMarkdownNoYaml":"\n<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n<script>\n  $(document).ready(function () {\n    $(\".abstract\").hide();\n    $(\".button\").on(\"click\", function () {\n        $(this).next(\".abstract\").slideToggle(400);\n    });\n});\n</script>\n\n<style>\n.abstract{text-align:justify; }\n.button{ text-align:justify; }\n</style>\n\n\n# Introduction\n\nThis handbook is dedicated for the next Data Specialist taking over the position within the Public Health Unit in the Global Programmes department at HQ. The book dives into each aspect of the position, showcasing step by step guidances on how to run the existing tools and how to maintain them. \n\nI will try my best to be as detailed as possible, but do know that other data/technical people will also have answers to all your questions.\n\n# Tools\n\n## impactR4PHU\n\n[healthyR](https://github.com/SaeedR1987/healthyr), a tool built by Saeed Rahman (current PHU Unit Manager), has been a crucial tool for checking, cleaning and analysing the data of almost all the outcome indicators of the different sectors of Public Health (Health/Nutrition/Food Security & Livelihoods - FSL/WASH). \n\n[impactR4PHU](https://github.com/impact-initiatives/impactR4PHU) followed the same original content structure of heathyR, but addressed some of the feedback from the users, as well as some other bugs/issues. The package also introduced project templates for fast implementations of the processes. \n\nThis package is following a [Test-Driven Development](https://www.techtarget.com/searchsoftwarequality/definition/test-driven-development) where unit testing, programming, and refactoring of the source code is addapted during development stages. \n\n### Content\n\nHere is a quick dissection of the GitHub Repo, but please do check the READ ME Section for a better description of each component. \n\n#### **main branch**\n\n##### **R Folder**\n\nThis folder includes all the functions that are used by the Public Health Unit for *creating*, *adding*, *checking*, and *ploting* data within different processes. Almost all functions have a test unit script using the [testthat](https://testthat.r-lib.org/) package. The tests can be access through the testing branch. Some of the functions are helpers functions and do not include testers. \n    \nAll functions are built in a way that can be piped within the **dplyr** system of piping.\n\nOnce you have checked all the functions, you might ask yourself a question. \n\n> *\"Where are the functions related to Health and WASH?\"* \n\nThey exist, but not in this package. At Impact, we love to share tasks sometimes, mainly when it is used for specific purposes in other units. You will be able to find these respective functions in the [humind](https://github.com/impact-initiatives-hppu/humind) package. The humind package is built to address main calculations for indicators collected through the MSNA (Multi-Sectoral Needs Assessment). To know more about this assessment or the package, please reach out to the HPPU Unit colleagues.\n\nAll functions are well documented using Roxygen, and some funcitons are running some examples using dummy data included in the package. \n\n##### **data Folder**\n\nThis folder includes different dummy datasets used within the examples inside the functions, but also within the test units. \n\n##### **inst Folder**\n\nThis folder includes different project templates:\n\n- FSL: Food Security and Livelihoods.\n- IYCF: Infant Youth Consumption Food TO CHECK\n- Mortality\n- IPHRA: Integrated Public Health Rapid Assessment\n- Public Health integrated tables\n\nYou might ask yourself another question here.\n\n> *Why do we have templates if there are already functions?*\n\nGood question!! But here is the answer.\n\nImpact's field missions are widely different when it come to data resources, capacity, and knowledge. Some missions do not even have data officers. So these templates exists to make the life of users with little knowledge of R easier and faster. They only need to set up R and R Studio on their machines, and run the respective scripts. \n\nThese project templates can easily be accessed through the \"New Project\" button in R Studio after installing the Package\n\n> devtools::install_github(\"impact-initiatives/impactR4PHU\")\n\nThe scripts interacts with the users through inputs system. Inputs here are very important to make sure that the scripts are outputing the correct intentions.\n\nPlease make sure to refer back to the respective specialists within the Public Health Unit so they can help you with what to select what when it comes to running the scripts.\n\n- Mortality: Noortje Gerritzma\n- FSL: Olivia Falkowitz\n- IYCF: Martin \n- IPHRA: PH team\n- PH Integrated Tables: PH team or Olivia. \n\nMortality, FSL, and IYCF have 3 templates:\n\n- Plausibility and Quality Reports\n- Cleaning\n- Descriptive analysis.\n\nIPHRA have 2 templates:\n\n- Cleaning\n- Analysis\n\nPH integrated Tables have only 1 template. \n\nAll the details needed are explained in the READ ME sections of the github page. \n\n#### **testing branch**\n\n##### **tests Folder** \n\nUsing the testthat package, and followint the TDD method, all functions have been tested following the good and bad path to make sure that they are outputing the expected results. Please do ensure to build tests always with every added functions, and feel free to add tests to existing functions. \n\n### Maintenance\n\nIf changes, improvements, enhancements, add-ons are required to the package, please use the following process to ensure a smooth merging to the main branch. \n\nFirst, clone the main package to your local machine and create a new branch for your work. \nIf functions are created/ammended, **please** make sure to checkout to the testing branch and do the necessary work there to keep the package trusted and correctly functioning. \n\nOnce done, go ahead commit and push your changes to your respective branch, and open a pull requests. \n\nOnce you create the pull request, some Github Actions will run in teh background to ensure that the package is functional and installable in different operating systems and the coverage test badge is updated. If all the actions pass the tests, I set a requirement here to have a review and validation by another person. However, as a solo data person in the unit, this can be bypassed and merge can be confirmed. \n\nGithub will again run another test of the Actions and pass a green badge to the R CMD Check. \n\n> The job is not yet done !!\n\nAs all the templates are using there own specific renv environment that also include the impactR4PHU github package, all these renv of the different templates should be updated to the newest version of the package. \n\nI created a python script that update them automatically instead of accessing them one by one. You will find the link to the python script inside the handover folder in the Sharepoint. \n\nOnce the script is done, please go again to your local branch where the new changes have been made and you will realise that all renv.lock files have now updated and require a new commit, push and pull for github. I know this is a bit of an annoying process, please go ahead and change it to your convenience. \n\n### Requirements\n\n- Please inform users to always re-install the package so they can access the latest version, especially if you are constantly enhancing/improving/changing something within the package. This is required because of the renv.locks that includes all newest packages requirements. \n\n- Each template have a renv::update() function in the beginning which is responsible of updating the relevant packages to the latest requirements for the dependencies. However, R sometimes love to be hard and try to break everything. I realized this after receiving some weird **\"make error\"** from the field. Kindly keep an eye on that. \n\n- Create a FAQ section compiled from the field.\n\n### Next Steps Suggestions\n\nAs the rotation within Impact is relatively high, you should consider having regular refreshers for missions/HQ Global Programs (GP) and Research Department (RDD) on the usage of the package and mainly the templates. \n\n> *\"Why GP and RDD included?\"*\n\nBecause, they do use the plausibility and quality report, mainly FSL, to check the data that is sent from missions for validation to HQ.\n\n> *Better way than R templates?*\n\nMy original plan was to create one whole system (software) that compile all the processes and be even more user friendly then still running R codes. However, this takes so much time to implement and we didn't have the time. \nSo maybe this is something that can be done as a project.\n\n### Knowledge requirements to run/maintain/enhance\n\n- **R:** *Advanced*\n- **Git and Github Actions:** *Advanced*\n- **Test-Driven Development:** *Advanced*\n- **Kobo:** *Intermediate*\n- **Python:** *Beginner*\n\n## RLC (Random Location Cluster) Sampling Scripts \n\nThis Github Repo is [LINK](https://github.com/AbrahamAz/rlc_sample) is dedicated for checking the quality of the data collection using the Random Location Cluster sampling or RLC for short as well as producing the weights for the different clusters. \n\nTo understand more about RLC, please check the presentations and the registrations available in the handover folder.\n\nIn a nutshell, RLC is a sampling method that requires to define a populated area, and randomly select N amount of GPS locations within this area. Once defined, the 3 nearest neighbouring HH to the defined GPS location are assessed. \n\nI created a Kobo tool called REACH GEO RAND that allows you to go to the field and trace the areas of interested and the tool will automatically generate N random points within this area with their respective UNIQUE IDs. Please check the presentation of the tool in the handover folder. \n\nThe output is an html report including:\n\n - *ID MATCHING*: Check Cluster_ID between sampling and rlc data are matching\n - *Clusters inside Area*: Check Clusters are inside the sampling area (50m threshold)\n - *Clusters follow RLC sampling*: Check assessed clusters are random and follow the sampling (50m threshold)\n - *HHs inside sampling area*: Check assessed HHs are inside the sampling are (50m threshold)\n - *Distance HHs to clusters*: Check the distance between the assessed HHs and the sampled cluster points (50m threshold)\n - *Duplicated clusters/HHs*: Check duplication between clusters and check if clusters share same HHs.\n - *Household Density*: Calculate HH Density & Cluster Weights\n - *IDW*: Spatial Interpolation using Inverse Distance Weighing (IDW)\n - *Estimation of Population*: Estimate Population of Sampled Area. (Under Revision)\n \n As well as an Excel file that includes the weights.\n\n### Requirements\n\nThis script are built using the REACH GEO RAND output data that consists of:\n\n- **Sheet including the traced areas**\n- **Sheet including the randomly generated points**\n\nAs well as your assessment data following the RLC sampling methodology that includes:\n\n- **UNIQUE IDs of the randomly generated points**\n- **GPS locations of the randomly generated points**\n- **GPS locations of the 3 assessed HHs**\n- **Number of individuals inside each HH**\n\n### Content\n\n#### **main branch**\n\n##### **data folder**\n\nThis folder includes some dummy data for testing the scripts. Both correct data and bad data.\n\n##### **output folder**\n\nThis folder includes the outputted HTML & Excel files of both the correct and wrong data. \n\n##### **resources folder**\n\nThis folder includes all the images/logos/local R packages needed to run the script in general. \n\n##### **src folder**\n\nThis folder includes multiple R scripts:\n\n- *init.R*: This script is used to install and load all the packages needed for this scripts. \n- *rlc_quality_check.R* & *rlc_quality_check_fast_track.R*: These are scripts that contains all the functions needed to perform the quality checks and create the outputs.\n\nAlso it includes a *utils folder* which includes all the other utility functions needed for cleaning/analysing/reporting. \n\n##### *main folder*\n\nThis folder includes multiple R, Rmd, and batch files. Below is the explanation of each:\n\n\n- **FAST TRACK**: \n  - *fast_track.R*: I automated the process to be able to run the whole script outside of R. this script is designated to initiate the project and render the Markdown. \n  - *fast_track.bat*: Batch file to run the whole process from outside of R. \n  - *rlc_sample_template_fast_track.Rmd*: The Markdown project that is called inside fast_track.R\n  \n### Maintenance\n\nThere are no maintenance needed for this projects. However, in case of errors, the file to be tracked is the rlc_sample_template_fast_track.Rmd, where you will be able to run each chunk and see where is the error happening. \n\n### Next Steps Suggestions\n\nOne suggestion that I advice for this project is the ability to not only be limited by the REACH GEO RAND as input for the defined area of the study. This will enable to check any RLC sampling methodology for sanity and quality of the data collection.\n\nTo change this, it will be required to find a way to provide the assessed households under the RLC Sampling methodology with a unique identifier for each cluster, and be able as well to define an area and input it in the script as a geometry polygon instead of the REACH GEO RAND. \n\nMost of the function inside src/rlc_quality_checks.R & src/rlc_quality_checks_fast_track.R will be affected by this change, as well as the markdown file. \n\n# Projects\n\n## Remote Mortality Studies in DRC\n\n### Content of the Repo\n\nThis project contains two main folders, the **Cleaning** and the **Analysis** folders. \n\nThe cleaning mainly target the informant method data to prepare it for the Analysis. \n\nThe Analysis folder will be our main focus in this part as we are using it to output all our analysis for the study. \n\nBelow is the full explanation of the folder structure.\n\n#### **data folder**\n\n- *adjust_wall.xlsx*: This is a small excel file created to clean the shelter component for the Network Survival data during the calculations of the socio-economic weighting. \n- *hh_survey_data.xlsx*: This is the HH survey long format data. \n- *Informant_method_full_data_2023_06_01.xlsx*: This is the cleaned informant method data outputed from the Cleaning folder. \n- *mortality_survey_clean_2023-09-16.xlsx*: This is the cleaned HH survey Data normal format (with all other loops included). Mainly used for the Network survival within HH. \n- *network_survival_data_clean.xlsx*: This is the cleaned Network Survival Data. \n- *Population DPS TANG023_ZS_AS_VILLAGE BICRS_revised.xlsx*: This is the Population Data for the ZS/AS used. \n\n#### **map folder**\n\n- *.csv/.xlsx*: These files are the outputs from the QGIS project to use for the Urban/Rural disaggregations. \n- *rural_urban.qgz*: QGIS Project file. \n- *Catchment_sites and Donnees_DRC_Tanganyika*: These folders include the shapefiles needed for the inputs to the QGIS project. \n\n#### **output folder**\n\nYou will see in this folder 3 main folders:\n\n- *Descriptive*: This folder includes all descriptive analysis (tables/figures/maps) of the 4 conducted mortality studies (HH survey/Informant Studies/Network Survival/Network Survival within HH survey)\n- *Evaluation*: This folder includes the comparative analysis done between all the methods.\n- *new_analysis*: This folder was created after the email shared by Saeed Rahman for the extra analysis requested. \n\n<div class='button' data-content=\"toggle-text\"><a href=\"#8\">Here are the set of extra analysis:</a></div>\n<div class='abstract'>\n<p style=\"margin-top: -0.1%;\">\n1)\tEffect of Recall Bias (Network Survival and Informant) – Deaths and Births\na.\tFor each round and method of data collection (HH survey, informant method, kin network, neighbour network remote, neighbour network on survey), please produce a basic table and histograms showing the number of deaths reported per month.\nb.\tGraphically, please produce 1 overall graph (histogram) per method, facet_wrap by round of reporting, and showing number of deaths reported per month. For the HH survey and Network Survival on HH survey, no need to facet_wrap by round of course.\ni.\tCasey showed us a similar breakdown, so what I would expect to see is a even distribution of deaths per month on the HH survey method, but a decrease in the early months from the remote methods showing a limitation in reporting. \nc.\tSeparately do the above also for births reported, though I imagine we can skip this for the informant method where the births were quite unclear. \nd.\tAnalyze for each remote method at which month the decline in deaths appears prominent, compared to the household survey. Re-produce the crude mortality and birth rates  \n</p>\n<p style=\"margin-top: -2.5%;\">\n2)\tEffects of Age of Death on Results (Network Survival and Informant) – It’s possible certain age groups like under-5 children had less We saw a similar graphic from Casey, with 5 year age group bins for crude mortality, which we should try to re-create for our reporting. Ideally we are seeing much higher rates in under-5 and older populations. Please produce basic tables and graphs. \na.\tGraphically, please produce a simple point graph of CDRs with confidence intervals, with the different age groups along the X-axis. Let’s produce the crude mortality for 5-year bins. For each method overall (HH survey, kin network, neighbor network remote, neighbor network on survey). \nb.\tAnalyze the results – when comparing the CDR by age group across methods, are there where the pattern differs significantly compared to the HH survey results? If so, consider which mortality rates for which age groups is most comparable to the HH survey results. Re-run the overall mortality results for the remote methods excluding the ill-fitting age groups. \n</p>\n<p style=\"margin-top: -2.5%;\">\n3)\tEffect of Rural/Urban on Results (Network Survival and Informant) – For both methods, to understand the effects of rural vs. urban sites on the effectiveness of the methods, for all methods. \na.\tPlease first identify and label for all methods (HH survey, network survival, ) which clusters, PSUs, or aire de santes can be considered rural vs. urban – harmonize across methods so we are comparing similar areas. I’d liaise with the DRC team on this, though broadly Kalemie and Nyunzu towns at most should be incorporating the urban areas. \nb.\tProduce birth and crude mortality rates overall for rural and urban areas for each of the methods, including HH survey for comparison. \nc.\tProduce table of incidence rate ratios and p-values for all comparisons here. \n</p>\n<p style=\"margin-top: -2.5%;\">\n4)\tEffects of Different Kin Relationships on Network Survival Birth and Death Rates (Network Survival Kin Tie Definition) - Specific to Network Survival Kin network for HH survey and remote methods, let’s please produce a basic tables and graphics to understand the distribution of reported deaths and births based on the kin relationship. There should be slightly more kin relationships in the HH survey network survival compared to the remote network survival. What I sort of expect to see here is that the more nebulous relationships like cousins, nieces, nephews, may have lower CDRs and birth rates which could be causing underestimation. \na.\tTabularly, produce a table with the number of each type of kin reported, total person time for that kin group, and numbers of births, number of crude deaths, and number of under-5 deaths. \nb.\tGraphically, for each method please produce a simple point graph of CDRs with confidence intervals by each of the kin relationships. \nc.\tSeparately, please produce a simple point graph of birth rates with confidence intervals by each of the kin relationships. \nd.\tAnalyze the results of the above graphs – are there any kin groups that appear to be significantly reporting fewer births or deaths per person-time observed? If so, remove from analysis and re-produce the overall birth and death rates without those groups for comparison. Produce table of incidence rate ratios and p-values for these new comparisons. \n</p>\n<p style=\"margin-top: -2.5%;\">\n5)\tEffects of Type of Key Informant on Informant Method Results (Informant Method) – both by the exact type, but also the broader categories of key informants that we grouped by (male leader, female leader, service provider)\na.\tPlease produce table summarizing the number of each exact type of KI interviewed, and the total number of households reported on, numbers of births, crude deaths, and under-5 deaths reported by each type. Disaggregate also by ZS, and by Rural/Urban.\nb.\tProduce a table showing the overlap in number and % in reporting between broad key informant types, consider the format of a 3x3 cross-tabulation with each of the 3 broad types along the horizontal and vertical axes.\nc.\tProduce a table showing the number of interviews, deaths and person-time reported per round for each broad type of key informant. \nd.\tGraphically, produce a Venn Diagram type graphic showing the overlap in number and % in reporting between different broad key informants types. See below an example. \n</p>\n<p style=\"margin-top: -2.5%;\">\n6)\tCapture-Recapture Analysis (Informant Method) – this will help us to determine the “completeness” of reporting of deaths by the informant method and give a proxy idea of the sensitivity of the method. I expect that overall it will be quite poor (50% or less sensitivity) but I suspect that when looking at rural areas alone, it will perform much better (80%+). \na.\tThis whole part I think is new to you, so let me know if you want to find some small time in the next few weeks to discuss together and walk through my past scripts. I’m sure there are a few improvements you can make in their efficiency as well. \nb.\tSome reference materials for you on capture-recapture and sensitivity\ni.\tConceptual and Assumptions of Capture Recapture - http://www.brixtonhealth.com/CRCaseFinding.pdf\nii.\tR coding, guidance, and data format requirements - https://hrdag.org/tech-notes/basic-mse.html\niii.\tPlease also find attached code from my previous implementations of capture-recapture for this method from some trials in SSD. \nc.\tPlease produce:\ni.\tTable summarizing unique deaths, estimated ‘uncaptured’ deaths, and estimated sensitivity. Include overall, but also disaggregated by ZS, by rural/urban. \nii.\tProduce and save graphics on the posterior probably estimates from the Bayesian Model Averaging. See an example below. \n</p>\n\n</div>\n\n\nInside the new_analysis folder, you will find two folders (graph / table) and inside them different numbered folders depending on the numbered requests.\n\n\n#### **resource folder**\n\n- *shapefil folder*: This folder include some shapefiles used inside the descriptive analysis folder. \n- *daf_ files.xlsx*: These excel files are dedicated for the some extra indicator analysis for all HH/Informant Study/and Network Survival. The indicators are causes/location of deaths etc. \n- *coverage_MSNA.xlsx*: Just an excel file showing the percentage of MSNA coverage in each Zone de sante\n- *death_removed.xlsx*: Excel files showing the death that were removed from the Informant study method after reviewing for duplications between methods. \n- *death.xlsx*: Excel files showing the death that remained from the Informant study method after reviewing for duplications between methods. \n- *.png*: files related for some logos/images.\n- *ns_round_weight.xlsx*: Excel file including the weight designed for each 2 month round of the Network Survival. \n- *quick_cleaning_ns.xlsx*: Some extra cleaning for the Network Survival Data\n- *quota_sampling_2023-03-01.xlsx & quota_sampling_2023-03-01withtabac.xlsx*: Excel files including the population and the population for each cluster_ID for the Informant Study method (one with Tabac Congo and one without).\n- *sampling_frame_.xlsx*: Sampling frames of each method. \n- *tool_.xlsx*: Kobo tool of each method\n- *weight_*: Weightings for each method. \n\n#### **src folder**\n\n- *utils folder*: include all the utility functions used inside the scripts\n- *convert_cols_with_daf.R & fix_bugged_names.R*: No need to worry about these scripts (also utility scripts for the extra descriptive analysis).\n- *format_dataset_.R*: These scripts is where I am transforming/modelling/calling some extra functions for each method. \n- *init_.R*: These scripts is where I am initiating each method for the analysis. \n- *weight_ses.R*: This script is dedicated for the calculation of the Socio Economic Weight for the Network Survival. \n- *new_analysis.R*: This script is for the new_analysis part asked by Saeed Rahman. \n\n#### **temp folder**\n\nOnly placeholder folder for the extra descriptive analysis. No need to worry about\n\n#### **main folder**\n\n- *run_analysis.R*: This script is to initiate the paths to each excel file and render the Markdown. \n- *analysis_tabular.Rmd*: Markdown where all the analysis is happening. Can be run chunk by chunk, or all together from the run_analysis.R\n- *mortalityremote.excalidraw*: If you go to [Excalidraw](https://excalidraw.com/), you will be able to upload this file and look at some of the notes taken for the new_analysis part. \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"embed-resources":true,"output-file":"handover.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":["simplex"],"title":"PHU Data Specialist Instruction/Guidelines Handbook","author":"Abraham Azar","date":"last-modified"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}